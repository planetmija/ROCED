Alternatively to the approach described above, the
open-source workload managing system \Slurm~\cite{Slurm} has been interfaced into \Roced by
the ATLAS group at University of Freiburg.
\Slurm provides a built-in functionality for the dynamic
startup of resources in the \textit{Slurm Elastic Computing}
module~\cite{SlurmElastic}.
However, this module is based on the assumption of a fixed maximum
startup time of the machines.
In the considered case, due to the queue in the host system, the start of a
resource can be delayed by a significant, varying time period.
In addition the transfer of information, such as error states, from one scheduler to the
other, and therefore to the user, is  limited.
Therefore, \Roced has been chosen as the interface between the
\Moab scheduler on the host system and the \Slurm
scheduler on the submission side.


\begin{figure}

\includegraphics[width=0.95\linewidth]{figures/virtualisierung_ROCED.png}
\caption{Implementation of \Roced with \Slurm on the Tier-3 cluster of
  the WLCG used by ATLAS researchers in Freiburg.}
\label{fig:slurmRocedBFG}
\end{figure}

The scheduling system is illustrated in Fig.~\ref{fig:slurmRocedBFG}.
For \Slurm, it is necessary that each potential virtual
machine is registered in the configuration at the time of start of the
\Slurm server as well as the client. \Slurm configurations also
need to be in agreement between server and client.
Therefore, a range of hostnames is registered in the configuration in
a way that is mapped to potential IP addresses of virtual machines.
These virtual machines have a fixed number of CPUs and memory assigned and are
registered under a certain \Slurm partition.
When a job is submitted to this partition and no other resource is
available, information from the \Slurm \texttt{squeue} and
\texttt{sinfo} commands is requested and parsed for the required information.

Since the ATLAS Freiburg group comprises three sub-groups, each mapped
to a different production account on \NEMO, special care is taken to
avoid interference of resources used by another account to ensure fair share on \NEMO, while
nevertheless allowing jobs from one group to occupy otherwise idle resources of another group.

\Roced determines the amount of virtual machines to be started and sends the
corresponding VRE job submission commands to \Moab.
After the virtual machine has booted, the hostname is set to the IP
dependent name which is known to the \Slurm configuration.
A \texttt{cron} job executes several sanity
checks on the system.
Upon successful execution of these tests, the \Slurm client
running in the VM starts accepting the queued jobs.
After completion of the jobs and a certain period of receiving no new jobs from the queue, the
\Slurm client in the machine drains itself and the machine
shuts itself down.
The IP address as well as the corresponding hostname in \Slurm
are released and can be reused by future VREs.
