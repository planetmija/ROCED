
Particle physics experiments at the Large Hadron Collider (LHC) need a
great quantity of computing resources for data processing, simulation, and analysis.
This demand will be growing with the upcoming High-Luminosity upgrade of the LHC~\cite{HLLHCcompneeds}.
To help fulfill this requirement, High Performance Computing (HPC) resources provided by research institutions
can be useful supplements to the existing World-wide LHC Computing
Grid (WLCG)~\cite{wlcg} resources
allocated by the collaborations.


This paper presents the concepts and implementation of providing a HPC resource, the
<<<<<<< HEAD
shared research cluster \NEMO~\cite{nemo} at the University of Freiburg, to ATLAS and CMS users accessing external clusters connected to the WLCG with the purpose of running data production as well as
=======
shared research cluster \NEMO at the University of Freiburg, to ATLAS and CMS users accessing external clusters connected to the WLCG with the purpose of accommodate data production as well as
>>>>>>> 8558eae79ded49a1b36aa82c14e8f56cca8dfd5a
data analysis on the HPC host system. The HPC cluster \NEMO at
the University of Freiburg is deploying an \Openstack~\cite{Openstack} instance to handle the
virtual machines. The challenge is in provisioning, setup, scheduling, and decommissioning the virtual research environments (VRE) dynamically and according to demand. For this purpose, the schedulers on \NEMO and on the external resources are
connected through the \Roced service~\cite{ROCED}.

%The CMS and ATLAS groups cooperated in this project with the HPC team of the
%eScience group of the computer center of University of Freiburg (UFR) to tackle
%the particular challenges of VREs like provisioning, setup, scheduling and
%decommissioning.
A VRE in the context of this paper is a complete software stack
as it would be installed on a compute cluster fitted to the demands of ATLAS or CMS workloads.
